diff --git a/3.test_cases/pytorch/torchtune/kubernetes/grpo.yaml b/3.test_cases/pytorch/torchtune/kubernetes/grpo.yaml
new file mode 100644
index 0000000..0a1af9b
--- /dev/null
+++ b/3.test_cases/pytorch/torchtune/kubernetes/grpo.yaml
@@ -0,0 +1,99 @@
+apiVersion: "kubeflow.org/v1"
+kind: PyTorchJob
+metadata:
+  name: qwen-grpo
+spec:
+  elasticPolicy:
+    rdzvBackend: c10d
+    minReplicas: 1
+    maxReplicas: 3
+    maxRestarts: 100
+    metrics:
+      - type: Resource
+        resource:
+          name: cpu
+          target:
+            type: Utilization
+            averageUtilization: 90
+  pytorchReplicaSpecs:
+    Worker:
+      replicas: 1
+      restartPolicy: OnFailure
+      template:
+        metadata:
+          labels:
+            app: qwen-grpo-0.6b
+        spec:
+          volumes:
+            - name: shmem
+              hostPath: 
+                path: /dev/shm
+            - name: local
+              hostPath:
+                path: /mnt/k8s-disks/0
+          #nodeSelector:
+          #  node.kubernetes.io/instance-type: "ml.g6e.8xlarge"
+          containers:
+            - name: pytorch
+              image: 166463094725.dkr.ecr.us-west-2.amazonaws.com/grpo:latest
+              imagePullPolicy: Always
+              resources:
+                requests:
+                  nvidia.com/gpu: 1
+                  vpc.amazonaws.com/efa: 1
+                limits:
+                  nvidia.com/gpu: 1
+                  vpc.amazonaws.com/efa: 1
+              env:
+              # for P5 FI_* should be commented out
+              - name: LOGLEVEL
+                value: "DEBUG"
+              #- name: FI_PROVIDER
+              #  value: efa
+              #- name: FI_EFA_USE_DEVICE_RDMA
+              #  value: "1"
+              #- name: FI_EFA_FORK_SAFE
+              #  value: "1"
+              #- name: FI_LOG_LEVEL
+              #  value: "1"
+              #- name: FI_EFA_ENABLE_SHM_TRANSFER
+              #  value: "1"
+              - name: TORCH_DISTRIBUTED_DEBUG
+                value: "DETAIL"
+              - name: TORCH_NCCL_ENABLE_MONITORING
+                value: "1"
+              - name: TORCH_NCCL_TRACE_BUFFER_SIZE
+                value: "20000"
+              - name: TORCH_NCCL_DUMP_ON_TIMEOUT
+                value: "1"
+              - name: TORCH_NCCL_DEBUG_INFO_TEMP_FILE
+                value: "/local/nccl_trace_rank_"
+              - name: PYTORCH_CUDA_ALLOC_CONF
+                value: "expandable_segments:True"
+              - name: NCCL_DEBUG
+                value: "INFO"
+              - name: NCCL_SOCKET_IFNAME
+                value: "^lo"
+              - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
+                value: "1"
+              - name: HF_TOKEN
+                value: "<HF_TOKEN>"
+              #- name: TORCH_DIST_INIT_BARRIER
+              #  value: "1"
+              #- name: NCCL_IGNORE_DISABLED_P2P
+              #  value: "1"
+              #- name: NCCL_NVLS_ENABLE
+              #  value: "0"
+              command:
+                - /bin/bash
+                - -c
+                - |
+                  if [ "" = "0" ]; then
+                    tune download Qwen/Qwen3-0.6B --output-dir /local/Qwen3-0.6B
+                  fi
+                  tune run --nproc_per_node 2 lora_finetune_distributed --config 0.6B_lora
+              volumeMounts:
+                - name: shmem
+                  mountPath: /dev/shm
+                - name: local
+                  mountPath: /local
\ No newline at end of file
